import json
import os
import re

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

# Importujemy TwÃ³j dziaÅ‚ajÄ…cy chain
from src.brain import get_rag_chain
from src.utils import get_config

# 1. Konfiguracja SÄ™dziego (Groq)
config = get_config()

# Pobieramy klucz API (z env lub configu)
api_key = os.getenv("GROQ_API_KEY") or config.get("groq_api_key")

judge_llm = ChatGroq(
    model_name=config["judge_model"],  # Llama 3 jest Å›wietna do bycia sÄ™dziÄ…
    temperature=0,  # Zero kreatywnoÅ›ci, sama logika
    api_key=api_key,
)


def extract_json_from_text(text):
    """WyciÄ…ga JSON nawet jak model doda coÅ› od siebie."""
    json_pattern = r"\{.*\}"
    match = re.search(json_pattern, text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group())
        except json.JSONDecodeError:
            pass

    # Fallback: szukanie rÄ™czne
    score_match = re.search(r'"score"\s*:\s*([01])', text)
    reason_match = re.search(r'"reason"\s*:\s*"([^"]*)"', text)

    if score_match and reason_match:
        return {"score": int(score_match.group(1)), "reason": reason_match.group(1)}

    return {"score": 0, "reason": "BÅ‚Ä…d parsowania odpowiedzi sÄ™dziego"}


def evaluate_faithfulness(answer, context_text):
    """
    Sprawdza WiernoÅ›Ä‡: Czy odpowiedÅº wynika z kontekstu?
    """
    prompt = ChatPromptTemplate.from_template("""
    JesteÅ› surowym sÄ™dziÄ… AI. Oceniasz "WiernoÅ›Ä‡" (Faithfulness).

    KONTEKST:
    {context}

    ODPOWIEDÅ¹ SYSTEMU:
    {answer}

    Zadanie: Czy odpowiedÅº wynika TYLKO z kontekstu? 
    1 = TAK (Wszystko jest w tekÅ›cie)
    0 = NIE (Model zmyÅ›la lub uÅ¼ywa wiedzy zewnÄ™trznej)

    Odpowiedz WYÅÄ„CZNIE w JSON: {{"score": <0 lub 1>, "reason": "<krÃ³tki powÃ³d>"}}
    """)

    chain = prompt | judge_llm | StrOutputParser()
    # Tutaj context_text jest juÅ¼ stringiem, wiÄ™c przekazujemy go bezpoÅ›rednio
    raw_response = chain.invoke({"answer": answer, "context": context_text})
    return extract_json_from_text(raw_response)


def evaluate_relevancy(question, answer):
    """
    Sprawdza TrafnoÅ›Ä‡: Czy odpowiedÅº jest na temat?
    """
    prompt = ChatPromptTemplate.from_template("""
    JesteÅ› surowym sÄ™dziÄ… AI. Oceniasz "TrafnoÅ›Ä‡" (Relevancy).

    PYTANIE: {question}
    ODPOWIEDÅ¹: {answer}

    Zadanie: Czy to jest odpowiedÅº na zadane pytanie?
    1 = TAK
    0 = NIE

    Odpowiedz WYÅÄ„CZNIE w JSON: {{"score": <0 lub 1>, "reason": "<krÃ³tki powÃ³d>"}}
    """)

    chain = prompt | judge_llm | StrOutputParser()
    raw_response = chain.invoke({"question": question, "answer": answer})
    return extract_json_from_text(raw_response)


def run_evaluation():
    print("\nğŸš€ START EWALUACJI (SÄ™dzia: Groq/Llama3)")
    print("-" * 50)

    # Zestaw pytaÅ„ testowych
    test_questions = [
        "Czym jest obiekt kosmiczny w Å›wietle prawa?",
        "Kto odpowiada za szkody wyrzÄ…dzone przez satelitÄ™ na Ziemi?",
        "Czy KsiÄ™Å¼yc moÅ¼e naleÅ¼eÄ‡ do prywatnej firmy?",
        "Jaki jest przepis na ciasto marchewkowe?",  # Test negatywny (Guardrails)
    ]

    rag_chain = get_rag_chain()

    total_faithfulness = 0
    total_relevancy = 0

    for q in test_questions:
        print(f"ğŸ” Pytanie: {q}")

        try:
            # 2. Uruchomienie Twojego Braina
            response = rag_chain.invoke({"question": q})

            answer = response["answer"]

            # --- KLUCZOWA POPRAWKA DLA CIEBIE ---
            # TwÃ³j brain.py zwraca "context" jako string (tekst), a nie listÄ™.
            # WiÄ™c po prostu go przypisujemy.
            context_text = response["context"]

            # Zabezpieczenie na wypadek pustego kontekstu
            if not context_text:
                context_text = "Brak kontekstu (pusty string)."

            # 3. Ocena SÄ™dziego
            faith_result = evaluate_faithfulness(answer, context_text)
            rel_result = evaluate_relevancy(q, answer)

            print(f"   ğŸ¤– OdpowiedÅº: {answer[:80]}...")
            print(
                f"   ğŸ›¡ï¸  WiernoÅ›Ä‡: {faith_result['score']} -> {faith_result['reason']}"
            )
            print(f"   ğŸ¯ TrafnoÅ›Ä‡: {rel_result['score']} -> {rel_result['reason']}")

            total_faithfulness += faith_result["score"]
            total_relevancy += rel_result["score"]

        except Exception as e:
            print(f"   âš ï¸ BÅ‚Ä…d oceny: {e}")

        print("-" * 50)

    # 4. Raport
    avg_faith = total_faithfulness / len(test_questions) if test_questions else 0
    avg_rel = total_relevancy / len(test_questions) if test_questions else 0

    print("\nğŸ“Š RAPORT KOÅƒCOWY:")
    print(f"Åšrednia WiernoÅ›Ä‡: {avg_faith:.2f} / 1.0")
    print(f"Åšrednia TrafnoÅ›Ä‡: {avg_rel:.2f} / 1.0")


if __name__ == "__main__":
    run_evaluation()
