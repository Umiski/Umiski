import os
import json
import re
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from src.brain import get_rag_chain
from src.utils import get_config

# 1. Konfiguracja Sƒôdziego (Groq)
# U≈ºywamy tego samego modelu co w brain.py, ale z temperaturƒÖ 0 dla powtarzalno≈õci ocen.
config = get_config()
judge_llm = ChatGroq(
    model_name=config["llm_model"],  # Np. llama3-70b-8192
    temperature=0,
    api_key=os.getenv("GROQ_API_KEY"),
)


def extract_json_from_text(text):
    """Extract JSON from text that may contain additional content"""
    # Look for JSON-like structure
    json_pattern = r"\{.*\}"
    match = re.search(json_pattern, text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group())
        except json.JSONDecodeError:
            pass

    # Try to find score and reason separately
    score_match = re.search(r'"score"\s*:\s*([01])', text)
    reason_match = re.search(r'"reason"\s*:\s*"([^"]*)"', text)

    if score_match and reason_match:
        return {"score": int(score_match.group(1)), "reason": reason_match.group(1)}

    # Fallback
    return {"score": 0, "reason": "Failed to parse response"}


def evaluate_faithfulness(answer, context_text):
    """
    Sprawdza Wierno≈õƒá (Faithfulness): Czy odpowied≈∫ wynika TYLKO z dostarczonych dokument√≥w?
    Chroni przed halucynacjami (zmy≈õlaniem fakt√≥w).
    """
    prompt = ChatPromptTemplate.from_template("""
    Jeste≈õ surowym sƒôdziƒÖ AI. Oceniasz "Wierno≈õƒá" (Faithfulness) odpowiedzi systemu RAG.

    KONTEKST (≈πr√≥d≈Ça wiedzy):
    {context}

    ODPOWIED≈π SYSTEMU:
    {answer}

    Zadanie: Przeanalizuj czy odpowied≈∫ wynika TYLKO z kontekstu. Je≈õli zawiera informacje spoza kontekstu, daj 0. Je≈õli wszystko jest oparte na kontek≈õcie, daj 1.

    Odpowiedz WY≈ÅƒÑCZNIE w formacie JSON, bez ≈ºadnego dodatkowego tekstu:
    {{"score": <0 lub 1>, "reason": "<kr√≥tki pow√≥d>"}}
    """)

    chain = prompt | judge_llm | StrOutputParser()
    raw_response = chain.invoke({"answer": answer, "context": context_text})
    return extract_json_from_text(raw_response)


def evaluate_relevancy(question, answer):
    """
    Sprawdza Trafno≈õƒá (Relevancy): Czy odpowied≈∫ faktycznie odpowiada na zadane pytanie?
    """
    prompt = ChatPromptTemplate.from_template("""
    Jeste≈õ surowym sƒôdziƒÖ AI. Oceniasz "Trafno≈õƒá" (Answer Relevancy).

    PYTANIE U≈ªYTKOWNIKA:
    {question}

    ODPOWIED≈π SYSTEMU:
    {answer}

    Zadanie: Oce≈Ñ czy odpowied≈∫ jest na temat pytania. Daj 1 je≈õli odpowied≈∫ dotyczy pytania, 0 je≈õli nie.

    Odpowiedz WY≈ÅƒÑCZNIE w formacie JSON, bez ≈ºadnego dodatkowego tekstu:
    {{"score": <0 lub 1>, "reason": "<kr√≥tki pow√≥d>"}}
    """)

    chain = prompt | judge_llm | StrOutputParser()
    raw_response = chain.invoke({"question": question, "answer": answer})
    return extract_json_from_text(raw_response)


def run_evaluation():
    print("\nüöÄ START EWALUACJI (Sƒôdzia: Groq/Llama3)")
    print("-" * 50)

    # Zestaw pyta≈Ñ testowych ("Golden Dataset")
    test_questions = [
        "Czym jest obiekt kosmiczny w ≈õwietle prawa?",
        "Kto odpowiada za szkody wyrzƒÖdzone przez satelitƒô na Ziemi?",
        "Czy Ksiƒô≈ºyc mo≈ºe nale≈ºeƒá do prywatnej firmy?",
        "Jaki jest przepis na ciasto marchewkowe?",  # Test negatywny (Guardrails)
    ]

    rag_chain = get_rag_chain()

    total_faithfulness = 0
    total_relevancy = 0
    results_log = []

    for q in test_questions:
        print(f"üîç Pytanie: {q}")

        # 2. Uruchomienie RAG (Twojego Braina)
        # UWAGA: brain.py wymaga klucza "question", nie "input"
        response = rag_chain.invoke({"question": q})

        answer = response["answer"]
        # WyciƒÖgamy tekst z dokument√≥w ≈∫r√≥d≈Çowych (context)
        context_docs = response["context"]
        context_text = "\n\n".join([doc.page_content for doc in context_docs])

        # 3. Ocena Sƒôdziego
        try:
            faith_result = evaluate_faithfulness(answer, context_text)
            rel_result = evaluate_relevancy(q, answer)

            # Logowanie wynik√≥w
            print(f"   ü§ñ Odpowied≈∫: {answer[:80]}...")
            print(
                f"   üõ°Ô∏è  Wierno≈õƒá (Faithfulness): {faith_result['score']} -> {faith_result['reason']}"
            )
            print(
                f"   üéØ Trafno≈õƒá (Relevancy):    {rel_result['score']} -> {rel_result['reason']}"
            )

            total_faithfulness += faith_result["score"]
            total_relevancy += rel_result["score"]

        except Exception as e:
            print(f"   ‚ö†Ô∏è B≈ÇƒÖd oceny dla tego pytania: {e}")

        print("-" * 50)

    # 4. Raport Ko≈Ñcowy
    avg_faith = total_faithfulness / len(test_questions)
    avg_rel = total_relevancy / len(test_questions)

    print("\nüìä RAPORT KO≈ÉCOWY:")
    print(f"≈örednia Wierno≈õƒá: {avg_faith:.2f} / 1.0")
    print(f"≈örednia Trafno≈õƒá: {avg_rel:.2f} / 1.0")

    if avg_faith < 0.8:
        print(
            "‚ö†Ô∏è SUGESTIA: Model halucynuje. Zwiƒôksz 'temperature' na 0 lub popraw Prompt Systemowy w brain.py."
        )
    if avg_rel < 0.8:
        print("‚ö†Ô∏è SUGESTIA: Model nie odpowiada wprost. Sprawd≈∫ retrieval_k w utils.py.")


if __name__ == "__main__":
    run_evaluation()
