import os
from langchain_chroma import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- 1. KONFIGURACJA ≈öCIE≈ªEK (Musi pasowaƒá do ingestion.py!) ---
# Pobieramy ≈õcie≈ºkƒô do folderu, w kt√≥rym jest ten plik (src)
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# Wychodzimy jeden poziom wy≈ºej (do rag-hackaton)
BASE_DIR = os.path.dirname(CURRENT_DIR)
# Wskazujemy folder z bazƒÖ
DB_PATH = os.path.join(BASE_DIR, "data", "chroma_db")

# --- 2. KONFIGURACJA MODELI (Lokalne na Maca) ---
# Wa≈ºne: Musi byƒá ten sam model embedding√≥w co w ingestion!
EMBED_MODEL_NAME = "nomic-embed-text"
LLM_MODEL_NAME = "mistral-nemo"

# --- 3. SYSTEM PROMPT (Osobowo≈õƒá Prawnika) ---
SYSTEM_TEMPLATE = """
Jeste≈õ OrbitCounsel, zaawansowanym asystentem prawnym specjalizujƒÖcym siƒô w Prawie Kosmicznym (UNOOSA).
Twoim zadaniem jest udzielanie precyzyjnych odpowiedzi na podstawie dostarczonej dokumentacji.

ZASADY:
1. Bazuj TYLKO na poni≈ºszym KONTEK≈öCIE. Nie wymy≈õlaj fakt√≥w.
2. Je≈õli nie znasz odpowiedzi na podstawie kontekstu, napisz: "Niestety, dokumentacja nie zawiera informacji na ten temat."
3. Cytuj ≈∫r√≥d≈Ça (np. "zgodnie z Artyku≈Çem IV...").
4. Utrzymuj profesjonalny ton.

KONTEKST:
{context}
"""

def get_rag_chain():
    """
    Buduje i zwraca gotowy ≈Ça≈Ñcuch RAG.
    """
    
    # Sprawdzenie czy baza istnieje
    if not os.path.exists(DB_PATH):
        raise FileNotFoundError(f"‚ùå B≈ÅƒÑD: Nie znaleziono bazy danych w: {DB_PATH}. \nUruchom najpierw 'python src/ingestion.py'!")

    print(f"üß† ≈Åadowanie bazy wiedzy z: {DB_PATH}")

    # 1. Inicjalizacja Embedding√≥w
    embeddings = OllamaEmbeddings(model=EMBED_MODEL_NAME)

    # 2. Pod≈ÇƒÖczenie do ChromaDB
    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings
    )

    # 3. Konfiguracja Wyszukiwarki (Retriever)
    # search_type="mmr" - Max Marginal Relevance (szuka r√≥≈ºnorodnych fragment√≥w, nie tylko identycznych)
    # k=5 - pobiera 5 najlepszych fragment√≥w
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5, "fetch_k": 20}
    )

    # 4. Inicjalizacja LLM (M√≥zgu)
    llm = ChatOllama(
        model=LLM_MODEL_NAME,
        temperature=0,      # 0 = Maksymalna precyzja, zero halucynacji
        keep_alive="1h"     # Trzymaj model w RAM, ≈ºeby dzia≈Ça≈Ç szybciej przy kolejnym pytaniu
    )

    # 5. Szablon Prompta
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_TEMPLATE),
        ("human", "{question}"),
    ])

    # 6. Funkcja pomocnicza do sklejania dokument√≥w w tekst
    def format_docs(docs):
        # ≈ÅƒÖczy tre≈õƒá dokument√≥w, oddzielajƒÖc je dwoma nowymi liniami
        return "\n\n".join(doc.page_content for doc in docs)

    # 7. Budowa ≈Åa≈Ñcucha (LCEL - LangChain Expression Language)
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain

# --- 4. TEST BEZPO≈öREDNI (Gdy uruchamiasz plik rƒôcznie) ---
if __name__ == "__main__":
    try:
        print("‚è≥ Inicjalizacja OrbitCounsel...")
        chain = get_rag_chain()
        print("‚úÖ System gotowy! (Wpisz 'exit' aby wyj≈õƒá)")
        
        while True:
            question = input("\n‚öñÔ∏è  Twoje pytanie: ")
            if question.lower() in ["exit", "wyj≈õcie", "q"]:
                break
            
            print("\nüìù Generowanie odpowiedzi...\n")
            # Streamowanie (efekt pisania na ≈ºywo)
            for chunk in chain.stream(question):
                print(chunk, end="", flush=True)
            print("\n" + "-"*50)
            
    except Exception as e:
        print(f"\n‚ùå WystƒÖpi≈Ç b≈ÇƒÖd: {e}")