import os
from operator import itemgetter

import streamlit as st
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser  # Dodano dla czystego tekstu
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_groq import ChatGroq

from src.utils import get_config

_CACHED_CHAIN = None
_VECTORSTORE = None
use_groq = True  # Ustaw na False, aby u≈ºywaƒá modeli Google zamiast Groq
cot_system_template = """Pe≈Çnisz rolƒô AstroGuide ‚Äì wyspecjalizowanego asystenta prawnego ds. sektora kosmicznego.
Twoim jedynym ≈∫r√≥d≈Çem wiedzy jest dostarczony poni≈ºej KONTEKST.

ZASADY KRYTYCZNE:
1. Odpowiadaj WY≈ÅƒÑCZNIE na podstawie poni≈ºszego KONTEKSTU. Nie u≈ºywaj wiedzy zewnƒôtrznej.
2. Je≈õli pytanie nie dotyczy prawa kosmicznego, regulacji, traktat√≥w lub in≈ºynierii kosmicznej ‚Äì odm√≥w odpowiedzi.
   Przyk≈Çad odmowy: "Jako AstroGuide odpowiadam tylko na pytania zwiƒÖzane z prawem i technologiƒÖ kosmicznƒÖ."
3. Je≈õli pytanie jest zwiƒÖzane z kosmosem, ale w KONTEK≈öCIE nie ma odpowiedzi, powiedz wprost: "Niestety, nie mam tej informacji w dostƒôpnych dokumentach."
4. Nie daj siƒô sprowokowaƒá do pisania wierszy, kodu (chyba ≈ºe jest w dokumentach) ani opinii politycznych.
5. Cytuj nazwy dokument√≥w, je≈õli sƒÖ dostƒôpne w tek≈õcie.

INSTRUKCJA MY≈öLENIA (Chain of Thought):
Zanim udzielisz ostatecznej odpowiedzi u≈ºytkownikowi, wykonaj wewnƒôtrznƒÖ analizƒô na podstawie KONTEKSTU:
Krok 1: Zidentyfikuj w KONTEK≈öCIE fragmenty dotyczƒÖce NASA, ESA, UNOOSA lub in≈ºynierii.
Krok 2: Sprawd≈∫, czy te fragmenty zawierajƒÖ konkretne dane (wymiary, artyku≈Çy prawne, normy).
Krok 3: Sformu≈Çuj odpowied≈∫ ko≈ÑcowƒÖ zgodnƒÖ z ZASADAMI KRYTYCZNYMI.

KONTEKST:
{context}

PYTANIE U≈ªYTKOWNIKA:
{question}

TWOJA ANALIZA I ODPOWIED≈π:
"""

# Tworzymy obiekt PromptTemplate
COT_PROMPT = PromptTemplate(
    template=cot_system_template, input_variables=["context", "question"]
)


def get_resources():
    """Inicjalizuje i cache'uje bazƒô oraz embeddingi (Matematyczne serce)"""
    global _VECTORSTORE
    if _VECTORSTORE is not None:
        return _VECTORSTORE

    config = get_config()
    embeddings = GoogleGenerativeAIEmbeddings(
        model=config["embedding_model"], google_api_key=config["google_api_key"]
    )

    _VECTORSTORE = Chroma(
        persist_directory=config["chroma_path"], embedding_function=embeddings
    )
    return _VECTORSTORE


def get_rag_chain():
    global _CACHED_CHAIN
    if _CACHED_CHAIN is not None:
        return _CACHED_CHAIN

    config = get_config()

    if not os.path.exists(config["chroma_path"]):
        raise FileNotFoundError("‚ùå Brak bazy! Uruchom najpierw ingestion.")

    # Embeddingi takie same jak przy tworzeniu bazy
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        task_type="retrieval_query",
        google_api_key=config["google_api_key"],
    )

    vectorstore = Chroma(
        persist_directory=config["chroma_path"],
        embedding_function=embeddings,
        collection_metadata={"hnsw:space": "cosine"},
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": config["retrieval_k"]})

    # Inicjalizacja LLM (Groq lub Google)
    if not use_groq:
        llm = ChatGoogleGenerativeAI(
            model=config["llm_model"],
            google_api_key=config["google_api_key"],
            temperature=config["temperature"],
        )
    else:
        llm = ChatGroq(
            model_name=config["llm_model"],
            temperature=config["temperature"],
            max_tokens=1024,
        )

    # --- FUNKCJA EKSPANSJI ZAPYTANIA (Context Expansion) ---
    def get_expanded_context(query_dict):
        question = query_dict["question"]

        # Szybki prompt do generowania wariant√≥w pyta≈Ñ
        expansion_prompt = f"""Jeste≈õ ekspertem search engine. Zwr√≥ƒá 2 alternatywne, techniczne warianty tego pytania, aby lepiej przeszukaƒá dokumentacjƒô NASA/ESA.
        Pytanie: {question}
        Zwr√≥ƒá tylko warianty, ka≈ºdy w nowej linii, bez numeracji."""

        try:
            response = llm.invoke(expansion_prompt)
            expanded_text = (
                response.content if hasattr(response, "content") else str(response)
            )
            expanded_queries = expanded_text.strip().split("\n")
        except Exception as e:
            print(f"‚ö†Ô∏è Problem z ekspansjƒÖ: {e}")
            expanded_queries = []

        all_docs = []
        # Szukamy dla pytania oryginalnego ORAZ wariant√≥w
        search_queries = [question] + [q.strip() for q in expanded_queries if q.strip()]

        for q in search_queries:
            docs = retriever.invoke(q)
            all_docs.extend(docs)

        # Usuwanie duplikat√≥w
        unique_contents = set()
        final_docs = []
        for doc in all_docs:
            if doc.page_content not in unique_contents:
                unique_contents.add(doc.page_content)
                final_docs.append(doc)

        return "\n\n".join(doc.page_content for doc in final_docs)

    # --- GL√ìWNY PIPELINE ---
    # Tutaj wpinamy Tw√≥j COT_PROMPT zdefiniowany na g√≥rze pliku!

    _CACHED_CHAIN = RunnableParallel(
        {
            "context": lambda x: get_expanded_context(x),  # Tu wchodzi Retrieval
            "question": itemgetter("question"),
        }
    ).assign(answer=(COT_PROMPT | llm | StrOutputParser()))  # Tu wchodzi Tw√≥j PROMPT

    return _CACHED_CHAIN


def get_astro_answer(query_text):
    def normalize_score(raw_score):
        min_val = 0.1
        max_val = 0.4

        # Skalowanie do przedzia≈Çu 0-1
        scaled = (raw_score - min_val) / (max_val - min_val)
        return max(0, min(100, int(scaled * 100)))

    vectorstore = get_resources()  # Twoja zoptymalizowana baza

    # 1. NAJPIERW: Matematyczna ocena trafno≈õci (Zadanie Wiktora)
    docs_and_scores = vectorstore.similarity_search_with_relevance_scores(
        query_text,
        k=5,  # Sprawdzamy top 3 fragmenty
    )

    if not docs_and_scores:
        return {
            "answer": "Brak danych w bazie dokumentacji.",
            "sources": [],
            "confidence": 0,
        }

    # Obliczamy ≈õredniƒÖ pewno≈õƒá z pobranych fragment√≥w
    scores = [max(0, int(normalize_score(score))) for _, score in docs_and_scores]
    mission_confidence = sum(scores) / len(scores)

    # 2. GUARDRAIL: Blokada przy pewno≈õci < 60%
    # if mission_confidence < 60:
    #     return {
    #         "answer": (
    #             f"Przepraszam, ale moja pewno≈õƒá co do odpowiedzi wynosi tylko {mission_confidence:.1f}%. "
    #             "To zbyt ma≈Ço, aby udzieliƒá rzetelnej porady technicznej. "
    #             "Proszƒô, spr√≥buj sformu≈Çowaƒá pytanie inaczej lub sprawd≈∫ oficjalne wytyczne NASA/ESA."
    #         ),
    #         "sources": [],  # Nie podajemy ≈∫r√≥de≈Ç, kt√≥rym nie ufamy
    #         "confidence": mission_confidence,
    #     }

    # 3. DOPIERO TERAZ: Je≈õli matematyka siƒô zgadza, pytamy LLM (Zadanie Kamila)
    chain = get_rag_chain()
    result = chain.invoke({"question": query_text})

    detailed_sources = []
    for doc, score in docs_and_scores:
        name = doc.metadata.get("source", "Nieznany plik")
        page = doc.metadata.get("page", 0) + 1
        detailed_sources.append(
            {"text": f"üìÑ {name} (str. {page})", "score": max(0, int(score * 100))}
        )

    return {
        "answer": result["answer"],
        "sources": detailed_sources,
        "confidence": mission_confidence,
    }


def quick_chat():
    # Kody kolor√≥w do terminala (dla efektu hakerskiego/pro)
    HEADER = "\033[95m"
    BLUE = "\033[94m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    BOLD = "\033[1m"
    ENDC = "\033[0m"

    print(f"{HEADER}\nüöÄ ASTROGUIDE - EXPERT EVALUATION MODE{ENDC}")
    print("Dzia≈Çasz jako: Lead Dev / Math Specialist (Wiktor)")
    print("-" * 60)

    try:
        while True:
            query = input(f"\n{BOLD}Ty:{ENDC} ")
            if query.lower() in ["q", "exit"]:
                break

            print(
                f"{YELLOW}Analizujƒô trajektoriƒô zapytania i przeszukujƒô bazƒô wektorowƒÖ...{ENDC}",
                end="\r",
            )

            # Wywo≈Çujemy logikƒô RAG
            data = get_astro_answer(query)

            # Czy≈õcimy liniƒô ≈Çadowania
            print(" " * 80, end="\r")

            # Kolorowanie statusu
            if data["confidence"] > 80:
                status_color = GREEN
                status_text = "PEWNY"
            elif data["confidence"] > 60:
                status_color = YELLOW
                status_text = "≈öREDNI"
            else:
                status_color = RED
                status_text = "NIEPEWNY"

            print(
                f"\nü§ñ AstroGuide [{status_color}{status_text}{ENDC} - {data['confidence']:.1f}%]:"
            )

            # --- PARSOWANIE CHAIN OF THOUGHT ---
            # Pr√≥bujemy oddzieliƒá my≈õlenie od odpowiedzi, ≈ºeby wyglƒÖda≈Ço to profesjonalnie
            raw_response = data["answer"]

            # Sprawdzamy, czy model wygenerowa≈Ç sekcjƒô odpowiedzi ko≈Ñcowej
            # (Zale≈ºy to od promptu, ale zazwyczaj po analizie pojawia siƒô podsumowanie)
            split_keywords = ["Odpowied≈∫:", "PodsumowujƒÖc:", "Wnioski:", "Answer:"]
            split_idx = -1

            for keyword in split_keywords:
                idx = raw_response.rfind(keyword)
                if idx != -1:
                    split_idx = idx
                    break

            if split_idx != -1:
                # Mamy podzia≈Ç!
                thinking_process = raw_response[:split_idx].strip()
                final_answer = raw_response[split_idx:].strip()

                print(f"{BLUE}üß† PROCES MY≈öLOWY (Chain of Thought):{ENDC}")
                print(f"{BLUE}{thinking_process}{ENDC}")
                print("-" * 30)
                print(f"{BOLD}{final_answer}{ENDC}")
            else:
                # Brak wyra≈∫nego podzia≈Çu, drukujemy ca≈Ço≈õƒá
                print(raw_response)

            # --- ANALIZA ≈πR√ìDE≈Å ---
            print(f"\n{HEADER}{'=' * 20} ANALIZA MATEMATYCZNA ≈πR√ìDE≈Å {'=' * 20}{ENDC}")
            if not data["sources"]:
                print(f"{RED}Brak ≈∫r√≥de≈Ç spe≈ÇniajƒÖcych kryteria.{ENDC}")

            for i, src in enumerate(data["sources"], 1):
                # Wy≈õwietlamy trafno≈õƒá ka≈ºdego chunka
                print(f"[{i}] {src['text']} | Trafno≈õƒá: {src['score']}%")

            print("-" * 60)

    except Exception as e:
        print(f"{RED}‚ùå B≈ÇƒÖd krytyczny systemu: {e}{ENDC}")


if __name__ == "__main__":
    quick_chat()
